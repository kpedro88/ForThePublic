+++
# About/Biography widget.
widget = "about"
active = true
date = 2016-04-20T00:00:00

# Order that this section will appear in.
weight = 5

# List your academic interests.
[interests]
  interests = [
    "Supersymmetry",
    "Dark Matter",
    "High Throughput Computing",
    "Big Data",
    "Machine Learning"
  ]

# List your qualifications (such as academic degrees).
[[education.courses]]
  course = "PhD in Particle Physics"
  institution = "University of Hamburg, Germany"
  year = 2005

[[education.courses]]
  course = "Diploma in Physics"
  institution = "University of Hamburg"
  year = 2001

+++



I am a particle physicist and am conducting **leading edge research** for New Physics Beyond the Standard Model of Particle Physics as well as precision Standard Model measurements.

I have multiple years of experience in analyzing high-energy collisions at different particle colliders using a multitude of different techniques. I have [published many papers in leading journals](https://github.com/gutsche/ForThePublic/raw/master/publication_list/complete_publication_list.pdf) and am currently a member of the CMS collaboration at the Large Hadron Collider (LHC) at [CERN](https://home.cern/). In my recent studies at the LHC, I have lead searches for evidence of physics beyond the Standard Model using top quarks, and contributed to searches for Supersymmetry and Dark Matter. One of my most noticeable publications is the [Observation of the Higgs Boson in 2012](https://doi.org/10.1016/j.physletb.2012.08.021).

I am a **leader in scientific computing** and have acquired deep knowledge and expertise in scientific software and computing infrastructure. My active involvement in HEP science allows me to guide the science community to benefit from the latest computing developments, bridging the worlds of science and IT.

Particle physics is based on particle detection by sophisticated experimental devices and their comparison to accurate simulations. Scientific software consists of millions of lines of C++ and python code and is needed to extract these physics results. I am an expert in object oriented software development, statistical data analysis methods and Monte Carlo simulation techniques as well as various optimization and machine learning techniques.

High Energy Physics (HEP) requires very large amounts of computing resources to analyze simulations and data recorded by the detectors. I have extensive experience in planning, developing, and operating distributed computing infrastructures that provide access to several hundred-thousand computing cores and hundreds of petabytes of disk space. I am intimately familiar with scientific grid sites, academic and commercial clouds and the largest U.S. supercomputers.

I was part of a worldwide community planning process for the software and computing infrastructure of the High Luminosity LHC (HL-LHC, 2026), which will require many times the computing resources as today. The goal of the community planning exercise was to provide the basis for an R&D program to reduce this increase significantly through disruptive changes to software and computing. Examples are new analysis paradigms using industry technologies, and vectorized and SIMD programming technologies and machine learning approaches to exploit new hardware architectures like accelerators and GPUs. I was an author for the [overview white paper of the community](http://arxiv.org/abs/1712.06982) and was co-editor of the topical white paper about the future of [data analysis in High Energy Physics](http://arxiv.org/abs/1804.03983).

My recent interest is to explore industry technologies for analysis of petabyte scale datasets. For this purpose, I started the [CMS Big Data Project](https://cms-big-data.github.io/). One component is a collaboration with Intel on scaling of analysis facilities to Petabytes. Fermilab recently joined [CERN openlab](https://openlab.cern/) to make this collaboration a possibility.

I held many management positions at the Fermi National Accelerator Laboratory and within the international CMS collaboration, supervising up to 100 individuals across many time zones. In September 2016, I was appointed U.S.CMS Software and Computing Operations Deputy Program manager overseeing the U.S. CMS Tier-1 and Tier-2 facilities as well as software maintenance and development for core software to computing infrastructure software to analysis systems. In this context, I am working together with DOE and NSF and partners at the universities and other labs to enable analysis of LHC particle collisions in the U.S. for the 2500 physicist strong CMS collaboration.

I regularly speak at interational conferences and workshops and am member of the editorial board of the journal for [Computing and Software for Big Science](https://www.springer.com/physics/particle+and+nuclear+physics/journal/41781?countryChanged=true).


----------

published on: 07. September 2018

----------
